{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 rows affected.\n",
      "0 rows affected.\n",
      "16 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Tables_in_stu2100013107</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Categories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Dept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Emp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>OrderDetails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Shippers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Suppliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>custInfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>happiness_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>orders</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('Categories',),\n",
       " ('Customers',),\n",
       " ('Dept',),\n",
       " ('Emp',),\n",
       " ('Employees',),\n",
       " ('OrderDetails',),\n",
       " ('Orders',),\n",
       " ('Products',),\n",
       " ('Shippers',),\n",
       " ('Suppliers',),\n",
       " ('accounts',),\n",
       " ('custInfo',),\n",
       " ('customers',),\n",
       " ('employees',),\n",
       " ('happiness_index',),\n",
       " ('orders',)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "mysql://stu2100013107:stu2100013107@162.105.146.37:43306\n",
    "show databases;\n",
    "use stu2100013107;\n",
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习二：基于SQL的数据挖掘算法\n",
    "\n",
    "鉴于机器学习的重要性，很多数据库都开始内置机器学习算法，比如SQL Server的DMX， PostGreSQL的Madlib， 这样可以直接对数据库里面的数 据执行机器学习算法。 我们这个小单元的练习目标是基于SQL实现典型的数据挖掘算法。 下面列出三个题目，题目一必做，题目二和题目三同学们完成其一即可。\n",
    "\n",
    "1. 熵和互信息\n",
    "2. 贝叶斯分类\n",
    "3. 决策树中属性的信息增益计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目一：熵和互信息的SQL实现\n",
    "\n",
    "1. 用SQL实现熵、 互信息的函数。 注意函数一般不能 接受表名作为参数进行传递， 但可以接受列名参数 ，所以可以针对一个固定的表来计算。\n",
    "2. 结合世界幸福报告数据集， 利用上面实现的函数， 探索各项指标对幸福指数的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicator: Social support, Mutual Information: 7.648263030901919\n",
      "Indicator: Generosity, Mutual Information: 7.648263030901919\n",
      "Indicator: Perceptions of corruption, Mutual Information: 7.648263030901919\n",
      "Indicator: Positive affect, Mutual Information: 7.648263030901919\n",
      "Indicator: Negative affect, Mutual Information: 7.648263030901919\n",
      "Indicator: Log GDP per capita, Mutual Information: 7.647601946323418\n",
      "Indicator: Freedom to make life choices, Mutual Information: 7.647601946323418\n",
      "Indicator: Healthy life expectancy at birth, Mutual Information: 6.713455958428286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/share/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/cluster/_supervised.py:65: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 数据库连接信息\n",
    "db_user = os.getenv('DB_USER', 'stu2100013107')\n",
    "db_password = os.getenv('DB_PASSWORD', 'stu2100013107')\n",
    "db_host = os.getenv('DB_HOST', '162.105.146.37')\n",
    "db_port = os.getenv('DB_PORT', '43306')\n",
    "db_name = os.getenv('DB_NAME', 'stu2100013107')\n",
    "\n",
    "# 创建数据库连接\n",
    "database_url = f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(database_url)\n",
    "data = pd.read_excel(\"世界幸福指数数据集.xls\")\n",
    "data.to_sql('happiness_index', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# 读取数据\n",
    "query = \"SELECT * FROM happiness_index\"\n",
    "df = pd.read_sql(query,engine)\n",
    "# 处理缺失值，可以选择删除或填充\n",
    "# 删除缺失值\n",
    "df = df.dropna()\n",
    "\n",
    "# 计算熵\n",
    "\n",
    "\n",
    "def entropy(series):\n",
    "    p_data = series.value_counts() / len(series)  # 概率数据\n",
    "    entropy = -sum(p_data * np.log2(p_data))  # 计算熵\n",
    "    return entropy\n",
    "\n",
    "# 计算互信息\n",
    "\n",
    "\n",
    "def calculate_mutual_information(df, column_x, column_y):\n",
    "    return mutual_info_score(df[column_x], df[column_y])\n",
    "\n",
    "\n",
    "# 各个指标与幸福指数的互信息\n",
    "columns = [\n",
    "    'Log GDP per capita', 'Social support', 'Healthy life expectancy at birth',\n",
    "    'Freedom to make life choices', 'Generosity', 'Perceptions of corruption',\n",
    "    'Positive affect', 'Negative affect'\n",
    "]\n",
    "\n",
    "mi_values = []\n",
    "for column in columns:\n",
    "    mi_value = calculate_mutual_information(df, column, 'Life Ladder')\n",
    "    mi_values.append((column, mi_value))\n",
    "\n",
    "# 按互信息值降序排序\n",
    "mi_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 输出结果\n",
    "for column, mi_value in mi_values:\n",
    "    print(f\"Indicator: {column}, Mutual Information: {mi_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 题目二：贝叶斯分类\n",
    "\n",
    "我们提供了一个buyComputer小表，里面有十几行顾客是否购买计算机的记录。基于这些记录，对于一个未知样本，采用贝叶斯分类算法，预测该样本是否会购买计算机。 你的实现形式应该是完成一个predict函数，其输入参数依次是样本的各项属性，输出为样本的类别。 下面是关于贝叶斯分类的简单介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先验概率:\n",
      "yes    0.642857\n",
      "no     0.357143\n",
      "Name: buys_computer, dtype: float64\n",
      "\n",
      "条件概率:\n",
      "类别 no:\n",
      "  P(age=30|no) = 0.6\n",
      "  P(age=50|no) = 0.39999999999999997\n",
      "  P(income=high|no) = 0.39999999999999997\n",
      "  P(income=low|no) = 0.19999999999999998\n",
      "  P(income=medium|no) = 0.39999999999999997\n",
      "  P(student=no|no) = 0.7999999999999999\n",
      "  P(student=yes|no) = 0.19999999999999998\n",
      "  P(credit_rating=excellent|no) = 0.6\n",
      "  P(credit_rating=fair|no) = 0.39999999999999997\n",
      "类别 yes:\n",
      "  P(age=30|yes) = 0.22222222222222218\n",
      "  P(age=40|yes) = 0.44444444444444436\n",
      "  P(age=50|yes) = 0.3333333333333333\n",
      "  P(income=high|yes) = 0.22222222222222218\n",
      "  P(income=low|yes) = 0.3333333333333333\n",
      "  P(income=medium|yes) = 0.44444444444444436\n",
      "  P(student=no|yes) = 0.3333333333333333\n",
      "  P(student=yes|yes) = 0.6666666666666666\n",
      "  P(credit_rating=excellent|yes) = 0.3333333333333333\n",
      "  P(credit_rating=fair|yes) = 0.6666666666666666\n",
      "\n",
      "后验概率计算:\n",
      "\n",
      "P(yes) = 0.6428571428571429\n",
      "P(age=30|yes) = 0.22222222222222218\n",
      "P(income=medium|yes) = 0.44444444444444436\n",
      "P(student=yes|yes) = 0.6666666666666666\n",
      "P(credit_rating=fair|yes) = 0.6666666666666666\n",
      "后验概率 P(yes|样本) = 0.02821869488536154\n",
      "\n",
      "P(no) = 0.35714285714285715\n",
      "P(age=30|no) = 0.6\n",
      "P(income=medium|no) = 0.39999999999999997\n",
      "P(student=yes|no) = 0.19999999999999998\n",
      "P(credit_rating=fair|no) = 0.39999999999999997\n",
      "后验概率 P(no|样本) = 0.006857142857142856\n",
      "\n",
      "最终预测: yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 创建数据集\n",
    "data = {\n",
    "    'age': [30, 30, 40, 50, 50, 50, 40, 30, 30, 50, 30, 40, 40, 50],\n",
    "    'income': ['high', 'high', 'high', 'medium', 'low', 'low', 'low', 'medium', 'low', 'medium', 'medium', 'medium', 'high', 'medium'],\n",
    "    'student': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no'],\n",
    "    'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'excellent'],\n",
    "    'buys_computer': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n",
    "}\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 计算先验概率\n",
    "\n",
    "\n",
    "def calculate_prior(df):\n",
    "    prior = df['buys_computer'].value_counts(normalize=True)\n",
    "    print(\"先验概率:\")\n",
    "    print(prior)\n",
    "    return prior\n",
    "\n",
    "# 计算条件概率的函数\n",
    "def calculate_likelihood(df):\n",
    "    likelihood = defaultdict(dict)\n",
    "    for feature in df.columns[:-1]:\n",
    "        feature_likelihood = df.groupby(['buys_computer', feature]).size().div(len(df))\n",
    "        buys_computer_prob = df['buys_computer'].value_counts().div(len(df))\n",
    "        \n",
    "        # 确保索引名称相同\n",
    "        buys_computer_prob.index.name = 'buys_computer'\n",
    "        \n",
    "        feature_likelihood = feature_likelihood.div(buys_computer_prob, axis=0, level='buys_computer')\n",
    "        for index, value in feature_likelihood.items():\n",
    "            likelihood[index[0]][(feature, index[1])] = value\n",
    "    \n",
    "    print(\"\\n条件概率:\")\n",
    "    for outcome, features in likelihood.items():\n",
    "        print(f\"类别 {outcome}:\")\n",
    "        for feature, prob in features.items():\n",
    "            print(f\"  P({feature[0]}={feature[1]}|{outcome}) = {prob}\")\n",
    "    return likelihood\n",
    "\n",
    "\n",
    "# 预测函数\n",
    "\n",
    "\n",
    "def predict(sample, prior, likelihood):\n",
    "    posterior = {}\n",
    "    print(\"\\n后验概率计算:\")\n",
    "    for outcome in prior.index:\n",
    "        posterior[outcome] = prior[outcome]\n",
    "        print(f\"\\nP({outcome}) = {posterior[outcome]}\")\n",
    "        for feature in sample:\n",
    "            prob = likelihood[outcome].get((feature, sample[feature]), 1e-6)  # 如果条件概率为0，使用一个很小的数代替\n",
    "            posterior[outcome] *= prob\n",
    "            print(f\"P({feature}={sample[feature]}|{outcome}) = {prob}\")\n",
    "        print(f\"后验概率 P({outcome}|样本) = {posterior[outcome]}\")\n",
    "    prediction = max(posterior, key=posterior.get)\n",
    "    print(f\"\\n最终预测: {prediction}\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# 计算先验概率和条件概率\n",
    "prior = calculate_prior(df)\n",
    "likelihood = calculate_likelihood(df)\n",
    "\n",
    "# 示例样本\n",
    "sample = {\n",
    "    'age': 30,\n",
    "    'income': 'medium',\n",
    "    'student': 'yes',\n",
    "    'credit_rating': 'fair'\n",
    "}\n",
    "\n",
    "# 预测\n",
    "prediction = predict(sample, prior, likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
